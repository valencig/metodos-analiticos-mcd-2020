---
title: "LSH para categorías de Wikipedia en Spark"
output: html_notebook
---


## Cargar datos en cluster

Normalmente, este paso no lo hacemos en nuestra sesión de análisis: los datos
están distribuidos en un cluster originalmente. Aunque podemos leer en este caso
los datos a R y después copiarlos a Spark, esto en general es poco eficiente
y a veces no es posible. Leeremos directamente al cluster.

Los datos completos están [aquí](https://s3.amazonaws.com/wiki-large/article_categories_en.ttl)

Iniciamos cluster un cluster local (en general nos conectamos a un cluster real):

```{r, message=FALSE}
library(tidyverse)
library(sparklyr)
#library(arrow)
config <- spark_config()
# configuración para modo local:
config$`sparklyr.shell.driver-memory` <- "2G" # para poder hacer collect de pares más adelante
```


```{r}
sc <- spark_connect(master = "local", config = config)
```


```{r}
options(scipen = 999)
ruta <- "../../datos/similitud/wiki-100000.txt"
articulos_tbl <- spark_read_csv(sc, 
  name = "articulos_df", 
  path = ruta, 
  columns = c("articulo", "categoria"),
  header =FALSE,
  delimiter = " ",
  repartition = 6) 
head(articulos_tbl)
articulos_tbl %>% tally()
```


```{r}
articulos_tbl <- articulos_tbl %>% 
  #filter(rlike(line, "^[^#]")) %>% 
  group_by(articulo) %>% 
  summarise(categorias = collect_list(categoria))
articulos_tbl %>% sdf_sample(0.001)
```

Y binarizamos (la representación para usar la implementación de spark es
de matriz rala: 1 cuando el token/shingle pertenece al documento, y 0 si no):

```{r}
art_bin <- articulos_tbl %>% 
        ft_count_vectorizer('categorias', 'vector_cat', binary = TRUE) 
art_bin %>% sdf_sample(0.001)
```


Ahora definimos el número de hashes

```{r}
# estimator
lsh_wiki_estimator <- ft_minhash_lsh(sc, "vector_cat", "hashes", 
                           seed = 1227,
                           num_hash_tables = 5)
```

```{r}
lsh_wiki_trans <-  ml_fit(lsh_wiki_estimator, art_bin)
art_bin <- ml_transform(lsh_wiki_trans, art_bin)
art_bin %>% head(5)
```

Podemos encontrar vecinos cercanos
```{r}
vec_1 <- art_bin %>% filter(articulo =='Alabama') %>% pull(vector_cat)
similares <- ml_approx_nearest_neighbors(lsh_wiki_trans, 
              art_bin, vec_1[[1]], num_nearest_neighbors = 10) %>% 
              select(articulo, categorias, distCol)
print(similares %>% collect)
```

Encontramos pares similares con un *similarity join*, por ejemplo:

```{r}
art_bin <- art_bin %>% mutate(id = articulo)
pares_candidatos <- ml_approx_similarity_join(lsh_wiki_trans, art_bin, art_bin, 0.7,
  dist_col = "distCol") %>% filter(id_a != id_b)
pares_candidatos  %>% tally()
```

```{r}
pares <- pares_candidatos %>% filter(distCol < 0.2)
pares %>% tally
pares <- pares %>% collect()
```

Por ejemplo

```{r}
DT::datatable(pares %>% filter(str_detect(id_a, "poker") | str_detect(id_b, "poker")))
```



Nota: la implementación en spark de LSH utiliza solamente amplificación OR. 
Es posible usar suficientes hashes para obtener pares, y después filtrar
los de la distancia que buscamos (¿Cómo implementar familias AND-OR)?

